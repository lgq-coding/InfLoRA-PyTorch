# InfLoRA-PyTorch
## 1、项目介绍
本项目是基于PyTorch的InfLoRA复现，是《人工智能实践课（初级）》的第一次复现（第二次汇报）内容。<br>
复现的论文是发表在 CVPR 2024 上的《InfLoRA: Interference-Free Low-Rank Adaptation for Continual Learning》，论文的GitHub链接为 https://github.com/liangyanshuo/InfLoRA ，论文的arxiv为 2404.00228 ，感谢作者对代码进行了开源，这为我的复现工作提供了非常重要的帮助，再次感谢！！！<br>
同时，也感谢《人工智能实践课》带给我的与众不同的体验，感谢在学习与复现过程中各位老师和同学的无私帮助！
## 2、论文介绍
以往的基于已有的PEFT(参数高效微调)的持续学习方法，要么用旧任务的参数适配新任务，要么随机扩展参数，然后适配新任务，这些方法都无法避免新任务对旧任务的干扰。本文引入了防干扰机制，注入一小部分参数，然后重参数化，并证明微调这一部分参数($A_t$)等价于在子空间内($B_t$的行向量的张成空间)微调预训练权重，并且消除了新任务对旧任务的干扰。<br>
精心设置了矩阵$B_t$: 与以往LoRA方法用高斯分布初始化不同，本文使用新任务梯度空间$N_t$和旧任务梯度空间的正交补空间$M_t$⊥的交集，保证$B_t$落在这个交集内，前者是为了保证学习新任务的能力(塑性)，而后者是为了不干扰旧任务的参数(固性)。<br>
在此基础上，每轮仅更新矩阵$A_t$(最开始矩阵A初始为0)，其余的均冻结，目标函数是local CE。<br>
空间的估计方法：①对于新任务，用新任务的输入矩阵估计梯度空间。②而旧任务的梯度空间信息，不能直接获取，通过DualGPM方法保存。③由于维度不一致，所以采取了奇异值分解和top-r奇异值选取的方法。<br>
本文的方法同样可以扩展到自监督学习的模型，还能结合类对齐方法。<br>
这篇文章运用线性代数知识，简单而优美，比较直观。
## 3、复现内容以及复现的心路历程
## 4、尝试复现本项目
## 5、实验结果与分析
## 6、进一步思考
